# Comparing-the-Risk-Performance-of-UMVU-and-MLE-Estimators-Under-Squared-Error-Loss

# ğŸ“Š Risk Comparison: UMVU vs MLE Estimators for Î¸(1 - Î¸)

This project compares the **UMVU (Uniformly Minimum Variance Unbiased)** estimator and the **MLE-based (Maximum Likelihood Estimator)** for estimating the quantity Î¸(1 - Î¸), where X follws Bin(n, Î¸). We explore how each estimator performs in terms of **risk** (mean squared error) under varying values of \(\theta\) and sample size \(n\).

---

## âœï¸ Estimators Being Compared

### âœ… UMVU Estimator (Unbiased)
The unbiased and minimum variance estimator among all unbiased estimators:
Î´*(X) = [X(n - X)] / [n(n - 1)]

### âœ… MLE-based Estimator (Biased)
Î´(X) = [X(n - X)] / nÂ²

The MLE is biased for finite samples but often has lower variance, which can lead to better overall performance in terms of risk.

---

## ğŸ” What This Project Does

- Computes the **risk** (mean squared error) of each estimator for:
  -  Î¸ = {0.05, 0.10, ...., 0.95}
  - n = {5, 15, 30, 45}
- Compares risks across estimators using:
  - Direct risk plots
  - Risk ratio plots Risk Ratio:  [ R(Î´*) ] / [ R(MLE) ]
  - A summary comparison table
- Shows how estimator performance changes as sample size increases

---

## ğŸ“ˆ Key Findings

- **MLE outperforms UMVU** at most \(\theta\) values when sample size is small
- **UMVU only performs better near \(\theta = 0.5\)** and becomes more competitive as n increases
- The **risk ratio plots** show that the UMVU estimator has lower risk than MLE only in the central region of Î¸, never across the full range

---



## ğŸ“Œ Requirements

This project uses standard R libraries:

- `ggplot2`
- `dplyr`
- `reshape2`
- `knitr`

You can install them using:

```r
install.packages(c("ggplot2", "dplyr", "reshape2", "knitr"))
